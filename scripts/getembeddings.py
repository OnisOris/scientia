import os
import numpy as np
import requests
from dotenv import load_dotenv
from sklearn.metrics.pairwise import cosine_similarity
import weaviate
from weaviate.auth import AuthApiKey
from weaviate.classes.config import Configure, DataType, Property, DistanceType
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
WEAVIATE_URL = os.getenv("WEAVIATE_URL")
WEAVIATE_API_KEY = os.getenv("WEAVIATE_API_KEY")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
if not OPENAI_API_KEY or not WEAVIATE_URL or not WEAVIATE_API_KEY:
    raise EnvironmentError("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ –≤ .env")

# –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
sentences = [
    "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ —Ç–≤–æ–∏ –¥–µ–ª–∞?",
    "–ü–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–∞—è!",
    "OpenAI –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏.",
]


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def get_openai_embeddings(texts):
    url = "https://api.openai.com/v1/embeddings"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": "text-embedding-3-small",
        "input": texts,
        "encoding_format": "float",
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code == 200:
        data = response.json()["data"]
        return [np.array(d["embedding"]) for d in data]
    else:
        raise RuntimeError(
            f"–û—à–∏–±–∫–∞ –æ—Ç OpenAI: {response.status_code} ‚Äî {response.text}"
        )


# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Weaviate
client = weaviate.connect_to_weaviate_cloud(
    cluster_url=WEAVIATE_URL, auth_credentials=AuthApiKey(WEAVIATE_API_KEY)
)

# –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π
class_name = "Sentence"

# –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
if client.collections.exists(class_name):
    client.collections.delete(class_name)

# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
collection = client.collections.create(
    name=class_name,
    vectorizer_config=Configure.Vectorizer.none(),
    properties=[
        Property(name="text", data_type=DataType.TEXT),
    ],
    vector_index_config=Configure.VectorIndex.hnsw(
        distance_metric=DistanceType.COSINE
    ),
)

# –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
embeddings = get_openai_embeddings(sentences)

# –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
for sentence, vector in zip(sentences, embeddings):
    collection.data.insert(
        properties={"text": sentence},
        vector=vector.tolist(),
    )

print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Weaviate")

# –¢–µ–ø–µ—Ä—å –ø–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã
collection = client.collections.get(class_name)


# –ü—Ä–∏–º–µ—Ä 1: –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
def semantic_search(query, top_k=3):
    print(f"\nüîç –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞: '{query}'")

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = get_openai_embeddings([query])[0].tolist()

    # –ò—â–µ–º –≤ Weaviate
    results = collection.query.near_vector(
        near_vector=query_embedding, limit=top_k, return_metadata=["distance"]
    )

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, obj in enumerate(results.objects):
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ —Å—Ö–æ–∂–µ—Å—Ç—å
        similarity = 1 - obj.metadata.distance
        print(f"\n#{i + 1}: {obj.properties['text']}")
        print(f"   –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {obj.metadata.distance:.4f}")
        print(f"   –°—Ö–æ–∂–µ—Å—Ç—å: {similarity:.4f}")


# –ü—Ä–∏–º–µ—Ä 2: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def visualize_embeddings():
    print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")

    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã
    all_objects = list(collection.iterator(include_vector=True))

    texts = []
    vectors = []

    for obj in all_objects:
        texts.append(obj.properties["text"])
        vectors.append(obj.vector["default"])

    vectors = np.array(vectors)

    # –°–Ω–∏–∂–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    pca = PCA(n_components=2)
    reduced_vectors = pca.fit_transform(vectors)

    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º
    plt.figure(figsize=(10, 8))
    for i, (x, y) in enumerate(reduced_vectors):
        plt.scatter(x, y, marker="o")
        plt.text(x + 0.01, y + 0.01, f"{i + 1}", fontsize=9)

    plt.title("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")

    # –î–æ–±–∞–≤–ª—è–µ–º –ª–µ–≥–µ–Ω–¥—É
    for i, text in enumerate(texts):
        print(f"{i + 1}: {text}")

    plt.grid()
    plt.show()


# –ü—Ä–∏–º–µ—Ä 3: –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏
def manual_similarity_calculation():
    print("\nüßÆ –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏")

    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã
    all_objects = list(collection.iterator(include_vector=True))

    if len(all_objects) < 2:
        print("–ù—É–∂–Ω–æ –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 –æ–±—ä–µ–∫—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return

    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ–±—ä–µ–∫—Ç—ã
    obj1 = all_objects[0]
    obj2 = all_objects[-1]

    vec1 = np.array(obj1.vector["default"])
    vec2 = np.array(obj2.vector["default"])

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
    similarity = cosine_similarity([vec1], [vec2])[0][0]

    print(f"–û–±—ä–µ–∫—Ç 1: '{obj1.properties['text']}'")
    print(f"–û–±—ä–µ–∫—Ç 2: '{obj2.properties['text']}'")
    print(f"–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.4f}")

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –µ–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    euclidean = np.linalg.norm(vec1 - vec2)
    print(f"–ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {euclidean:.4f}")


# –ü—Ä–∏–º–µ—Ä 4: –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
def hybrid_search(query, keyword=None):
    print(f"\nüîç –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: '{query}'")

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = get_openai_embeddings([query])[0].tolist()

    # –°—Ç—Ä–æ–∏–º —Ñ–∏–ª—å—Ç—Ä
    filter = None
    if keyword:
        filter = weaviate.classes.query.Filter.by_property("text").like(
            f"*{keyword}*"
        )

    # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
    results = collection.query.near_vector(
        near_vector=query_embedding,
        limit=3,
        filters=filter,
        return_metadata=["distance"],
    )

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, obj in enumerate(results.objects):
        similarity = 1 - obj.metadata.distance
        print(f"\n#{i + 1}: {obj.properties['text']}")
        print(f"   –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {obj.metadata.distance:.4f}")
        print(f"   –°—Ö–æ–∂–µ—Å—Ç—å: {similarity:.4f}")


# –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
if __name__ == "__main__":
    try:
        # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–∞
        semantic_search("–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ?")
        semantic_search("–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç")
        semantic_search("OpenAI –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ—â–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏.")

        # –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç
        manual_similarity_calculation()

        # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        hybrid_search("–ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ", keyword="OpenAI")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        visualize_embeddings()
    finally:
        # –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        client.close()
